## Hi We are Open X-Humanoid ðŸ‘‹
![logo](./static/logo.png)

The Beijing Humanoid Robot Innovation Center was established in November 2023 in Beijing Economic and Technological Development Zone (Beijing Yizhuang). It is the first innovation center in China that focuses on the core technology, product development, and application ecosystem construction of humanoid robots.

[![çŸ¥ä¹Ž](https://img.shields.io/badge/çŸ¥ä¹Ž-ä¸»é¡µ-blue?logo=zhihu&logoColor=white)](https://www.zhihu.com/people/85-97-42-12)
[![Email](https://img.shields.io/badge/Email-x--humanoid-red?logo=gmail&logoColor=white)](mailto:github@x-humanoid.com)
[![Website](https://img.shields.io/badge/Website-OpenSource-green?logo=internet-explorer&logoColor=white)](https://opensource.x-humanoid-cloud.com/)
[![Hugging Face Datasets](https://img.shields.io/badge/Hugging%20Face-Datasets-yellow)](https://huggingface.co/datasets/x-humanoid-robomind/RoboMIND)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue)](https://opensource.org/licenses/MIT)


# Join Our Open Source Community!â€‹
Calling all developers worldwide! Whatever industry you're from, we'd love for you to contribute to our open source projects for embodied AI. Come help us build and enhance this ecosystem together.

Hereâ€™s how you can get involved:

- â€‹â€‹Explore & Contribute:â€‹â€‹ Dive into our projects. Give them a good try! If you find an issue, tell us. Even better â€“ we'd be thrilled if you submit a PR fix!  
- â€‹â€‹Share Your Knowledge:â€‹â€‹ Write technical articles about using embodied AI. Check out our ongoing [RoboMIND Use Case Campaign for details](https://opensource.x-humanoid-cloud.com/forum.php?mod=viewthread&tid=216&extra=page%3D1) â€“ you could win generous prizes!
- â€‹â€‹Partner With Us:â€‹â€‹ Looking to collaborate? Universities and companies â€“ we're open to ideas! If you've got a proposal, reach out and let's talk.

# Contact Us
- opensource@x-humanoid.com

# Repositories
<table><tbody>

<table class="table table-striped table-bordered table-vcenter"/>
    <tbody>
    <tr><th> Title </th> <th>Description</th>
    <tr>
       <td align="center" > <a href="https://x-humanoid-robomind.github.io/">RoboMIND</a></td>
        <td>  a comprehensive dataset featuring 107k real-world demonstration trajectories spanning 479 distinct tasks and involving 96 unique object classes.<br></a></td>
     <tr>
         <td align="center" > <a href="https://github.com/Open-X-Humanoid/x-humanoid-training-toolchain">x-humanoid-training-toolchain</a></td>
        <td>This project provides a training toolchain for adapting TienKung humanoid robots and RoboMIND dataset with the open-source LeRobot framework. It enables users to facilitates development using RoboMIND dataset and train embodied manipulation models for TienKung robots based on the Lerobot.<br></a></td>
    </tr>
    <tr>
         <td align="center" > <a href="https://github.com/Open-X-Humanoid/TienKung_URDF">TienKung_URDF</a></td>
        <td>urdf publish is the URDF package for Tien Kung,which includes complete robot description files (URDF) and mesh files(STL), defining core parameters such as mechanical structure, joint limits,and mass distribution.lt supports motion planning and control algorithmverification in the ROS environment and Gazebo simulation platform.<br></a></td>
    </tr>
     <tr>
          <td align="center" > <a href="https://github.com/Open-X-Humanoid/TienKung_ROS">TienKung_ROS</a></td>
        <td>The Tien Kung software system, developed based on the ROS frameworkis the low-level implementation directly responsible for hardware controlincluding key modules such as body control (body_control),robot description (robot_description), andremote control communication (usb sbus), responsible for the basic motioncontrol and hardware driving of the robot.<br></a></td>
    </tr>
    <tr>
          <td align="center" > <a href="https://github.com/Open-X-Humanoid/TienKung_Docs">TienKung_Docs</a></td>
        <td> User manuals and SDK documentation for the TienKung, including both the Lite and Pro versions, covering robot unboxing, daily usage, maintenance guidelines, and SDK interface instructions.<br></a></td>
    </tr>
     <tr>
          <td align="center" > <a href="https://huggingface.co/datasets/x-humanoid-robomind/ArtVIP">ArtVIP</a></td>
        <td> We release a collection of 26 categories, 206 high-quality digital-twin articulated objects.We provide digital-twin scene assets and configured scenarios integrating articulated objects within scene for immediate use.All assets are provided in USD format and are open-source.The detailed production process and standard offer comprehensive guidance to facilitate community adoption and replication.<br></a></td>
    </tr>
     <tr>
          <td align="center" > <a href="https://switchvla.github.io/">SwitchVLA</a></td>
        <td> a unified execution-aware framework for Vision-Language-Action (VLA) robots. By conditioning action generation on both task language and fine-grained execution signals, SwitchVLA enables seamless transitions across forward, rollback, and advance behaviorsâ€”without relying on additional demonstration data, modular planners or handcrafted switching logic. SwitchVLA serves as a more generalizable solution for instruction-conditioned controlâ€”capable of unifying diverse switching behaviors within a single policy framework.<br></a></td>
    </tr>
    </tr>
    </tbody>
</table>


# Developer Community
<img src="./static/qrcode.png" border=0 width=30%>

